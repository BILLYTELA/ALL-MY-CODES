{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTMcBd//KAD9HpXEDPWCjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BILLYTELA/ALL-MY-CODES/blob/main/GEN_AI_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.TEXT SUMMARIZATION"
      ],
      "metadata": {
        "id": "4b2sKG5M1XyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "14rEIETwzGR5"
      },
      "outputs": [],
      "source": [
        "#import the library\n",
        "# We do not need a tokenizer for this\n",
        "\n",
        "from transformers import pipeline\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2, Our model as a summarizer model (initialize)\n",
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yp1eLeW0Dcs",
        "outputId": "3ed49fba-500a-4bd0-f6d0-2ba06276d8a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample text to be summarized\n",
        "text = \"\"\" The emergence of artificial intelligence has\n",
        "reshaped sectors and drastically altered our interaction with\n",
        "technology. AI-powered solutions are now ubiquitous in\n",
        "various sectors, including healthcare, finance, and\n",
        "transportation. These advancements have led to improved\n",
        "efficiency, accuracy, and innovation. However, concerns\n",
        "about AI ethics, bias, and job displacement remain. It is\n",
        "essential to address these challenges while harnessing the\n",
        "potential\"\"\""
      ],
      "metadata": {
        "id": "Wy3Cn-L10dri"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(text, max_length=50, min_length=30, do_sample=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zGOoKiX206Ay"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.CODE COMPLETION"
      ],
      "metadata": {
        "id": "AtQxcQ9M1-7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dA5pny8R2Cnl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a pre-trained model\n",
        "\n",
        "code_completion = pipeline(\"text-generation\", model=\"Salesforce/codegen-350M-mono\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpkQLIQz2JDe",
        "outputId": "cf19f2e2-94ab-432a-b2c0-b608ae7b1f33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"use python,def password_cracker(password):\""
      ],
      "metadata": {
        "id": "rDq-TE-B2tnj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completed_code = code_completion(prompt, max_length=250, num_return_sequences=1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHoVJsWe3HeU",
        "outputId": "360a921b-f928-41b4-dab3-043cc36e4668"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completed_code[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI_JZstZ3xRy",
        "outputId": "27ce2774-bd62-4eee-a6ca-1ce588eaec72"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use python,def password_cracker(password):\r\n",
            "##    pass\r\n",
            "\r\n",
            "##  Challenge 1: Take one word  and replace it with a special character.\r\n",
            "##  >>>password_cracker(\"PwC3dE\")\r\n",
            "##  'p!3DcE'\r\n",
            "\r\n",
            "## challenge 2: Take two words and create a new word out of them.\r\n",
            "##>>>password_cracker(\"pwC3dE!\")\r\n",
            "##'pwC3eE'\r\n",
            "## The password generated is a simple password and the word is now in the word list.\r\n",
            "##\r\n",
            "## challenge 3: Take string, replace it with another string, use the.replace() method and return the output\r\n",
            "##          for the password and print it and it will also return the index of the string that they match.\r\n",
            "##\r\n",
            "## challenge 4: Take a string, use re and compare it against a string in another string, keep only the first 10 characters\r\n",
            "##          of it, and print it out. For example if word is 3rd to end with letter and string is 'pwC3!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.TEXT GENERATION LLM"
      ],
      "metadata": {
        "id": "KH_5tVXd4xKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "dWhFPCmA45SG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"Once Upon A time, in a far away kingdom\""
      ],
      "metadata": {
        "id": "IoOD2FGs5PYW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generator(prompt, max_length=200,do_sample =True, temperature=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_ZHCqwO54XH",
        "outputId": "9aa794b1-dee9-4e7d-8944-9b977832093b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5fhzKyd6Oo-",
        "outputId": "0d0b2e06-5460-447c-edb2-89fbc5d944c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once Upon A time, in a far away kingdom, there were many who were so honored with the knowledge of God that they had to be called to this holy place. And they would come there and worship Him, and to the great and great knowledge of God. And as they would come there they would be called to the great knowledge of God; and they would be called to the great knowledge of God. And as they would come there they would be called to the great knowledge of God; and they would be called to the great knowledge of God. And as they would come there they would be called to the great knowledge of God; and they would be called to the great knowledge of God. And as they would come there they would be called to the great knowledge of God; and they would be called to the great knowledge of God. And as they would come there they would be called to the great knowledge of God; and they would be called to the great knowledge of God. And as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEXT CLASS\n",
        "#TEXT TO IMAGE\n",
        "#PROMPT ENGINEERING\n",
        "#MILESTONE LLM: OPENAI, GEMINI, LLAMA ...How to incoporate them in your application\n",
        "#Fine Tuning\n",
        "#RAG\n",
        "\n",
        "#Research On:\n",
        "#StreamLit Python Library"
      ],
      "metadata": {
        "id": "l40B7vaf6rzT"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}